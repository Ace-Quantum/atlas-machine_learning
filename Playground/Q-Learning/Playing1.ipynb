{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is from this tutorial:\n",
    "https://www.geeksforgeeks.org/q-learning-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the environment\n",
    "import numpy as np\n",
    "\n",
    "# States in the grid world\n",
    "#   wtf is a grid world\n",
    "n_states = 16 \n",
    "\n",
    "# Possible Actions (up, down, left, right)\n",
    "n_actions = 4\n",
    "\n",
    "# Goal State\n",
    "#   wtf is this??? We have a goal for what state we end up in? \n",
    "#   What is it out of???\n",
    "goal_state = 15\n",
    "\n",
    "# Q-table initialized with zeros\n",
    "Q_table = np.zeros((n_states, n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up hyperparameters\n",
    "\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "exploration_prob = 0.2\n",
    "epochs = 1000\n",
    "\n",
    "# I only know what two of these parameters are.\n",
    "# The other two seem specific to this concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up next is implimenting the Q-learning Alg. We're going to go through multiple epochs and selecting actions based on the epsilon-greedy strategy. What's the epsilon-greedy strategy? No idea! The human brain is not built for memory, therefor I cannot remember the big chonky words I read 10 minutes ago. \n",
    "\n",
    "Anyway, we also update the Q-values based on rewards and transition to the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q learning Alg\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # starting from a random state\n",
    "    current_state = np.random.randint(0, n_states)\n",
    "\n",
    "    while current_state != goal_state:\n",
    "        #   Ok I think I kinda get it now\n",
    "        # Choose action with epsilon-greedy strategy\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            # Explore\n",
    "            #   wtf do you mean explore???\n",
    "            action = np.random.randint(0, n_actions)\n",
    "        else:\n",
    "            # Exploit\n",
    "            #   ??????\n",
    "            #   What are we exploiting????\n",
    "            action = np.argmax(Q_table[current_state])\n",
    "\n",
    "        # Simulate environment / move to the next state\n",
    "        #   uhhhhhh\n",
    "        # Simply put, move to the next state\n",
    "        #   None of this is simple.\n",
    "        #       Ok now that I have things indented correctly \n",
    "        #       things make more sense\n",
    "        next_state = (current_state + 1) % n_states\n",
    "        #   Wow it's been a hot second since I've used modulo\n",
    "\n",
    "        # define simple reward function\n",
    "        reward = 1 if next_state == goal_state else 0\n",
    "\n",
    "        # Update the Q-learning update rule\n",
    "        #   Yep those words certainly made sense\n",
    "        Q_table[current_state, action] += learning_rate * (\n",
    "            reward + discount_factor * np.max(\n",
    "                Q_table[next_state]) - Q_table[current_state, action])\n",
    "        \n",
    "        # move to the next state\n",
    "        current_state = next_state\n",
    "        #   It is WILD seeing such a short line of code \n",
    "        #   after that chonky af math equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[0.48767498 0.48751286 0.48751892 0.48377358]\n",
      " [0.50923535 0.51330923 0.51334077 0.51334208]\n",
      " [0.54036009 0.54036009 0.54036009 0.54035981]\n",
      " [0.56880009 0.56880009 0.56880008 0.56880009]\n",
      " [0.59873694 0.59873541 0.59873541 0.59873663]\n",
      " [0.63024941 0.63024941 0.63024941 0.63024941]\n",
      " [0.66342043 0.66342043 0.66342043 0.66342043]\n",
      " [0.6983373  0.6983373  0.6983373  0.6983373 ]\n",
      " [0.73509189 0.73509189 0.73509189 0.73509189]\n",
      " [0.77378094 0.77378094 0.77378094 0.77378094]\n",
      " [0.81450625 0.81450625 0.81450625 0.81450625]\n",
      " [0.857375   0.857375   0.857375   0.857375  ]\n",
      " [0.9025     0.9025     0.9025     0.9025    ]\n",
      " [0.95       0.95       0.95       0.95      ]\n",
      " [1.         1.         1.         1.        ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#   Ok there's no way that last code block is supposed to be able \n",
    "#   to be ran in .1 secconds\n",
    "\n",
    "# After training the Q-table should represent the learned Q-Values\n",
    "print(\"Learned Q-table:\")\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so what did I accomplish though? This is saying that it produces a model to address certain issues, but we didn't create a model. Only a table. Is this like... preliminary data processing then?\n",
    "\n",
    "Looks like that's a no. It's used to build neural networks, but it in and of itself is not a neural network\n",
    "\n",
    "I need a use case to visualize it. Numbers only mean so much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
